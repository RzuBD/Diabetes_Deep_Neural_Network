{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c893df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import tensorflow library for deep neural network.\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a88768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we prepare our directories\n",
    "mother_directory = os.getcwd()\n",
    "data_directory = \"../input/early-diabetes-classification/diabetes_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ca468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_binary(data,categorical_dict):\n",
    "    sütun_list = []\n",
    "    for sütun in data.columns:\n",
    "        if type(data[sütun].iloc[0]) is str:\n",
    "            çeşitler = data[sütun].unique()\n",
    "            sütun_list.append(sütun)\n",
    "            for kaş in range(len(çeşitler)-1):\n",
    "                categorical_dict[çeşitler[kaş]] = []\n",
    "                for j in range(len(data[sütun])):\n",
    "                    if data[sütun].iloc[j] == çeşitler[kaş]:\n",
    "                        categorical_dict[çeşitler[kaş]].append(1)\n",
    "                    else:\n",
    "                        categorical_dict[çeşitler[kaş]].append(0)\n",
    "\n",
    "    return sütun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we grab our data into a dataframe.\n",
    "data = pd.read_csv(data_directory,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we turn categorical into one hot encoded columns\n",
    "categorical_dict = {}\n",
    "liste_sütun = categorical_to_binary(data,categorical_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we change the data into one hot encoded\n",
    "for name in categorical_dict.keys():\n",
    "    data[name] = categorical_dict[name]\n",
    "for k in liste_sütun:\n",
    "    data = data.drop(k,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map to look at the correlations between the data categories\n",
    "tc = data.corr()\n",
    "sns.heatmap(tc,annot = False,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66794a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we prepare data adn train it on the neural network we defined.\n",
    "malum = data[\"obesity\"].copy()\n",
    "data = data.drop(\"obesity\",axis=1)\n",
    "X = data.iloc[:,:].values\n",
    "y = malum.values\n",
    "\n",
    "#Here split our data into train_test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.15,random_state = 10)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "yedek = y_test.copy()\n",
    "\n",
    "#here we standardize the data into the same interval so that a category wouldnt overrepresent in the decision process.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)\n",
    "\n",
    "#We define our neural network and compile it\n",
    "classifier = tf.keras.models.Sequential()\n",
    "classifier.add(tf.keras.layers.Dense(units = 128, activation = \"relu\"))\n",
    "classifier.add(tf.keras.layers.Dropout(0.2))\n",
    "classifier.add(tf.keras.layers.Dense(units = 32, activation = \"relu\"))\n",
    "classifier.add(tf.keras.layers.Dropout(0.1))\n",
    "classifier.add(tf.keras.layers.Dense(units = 1,activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\" , metrics=[\"accuracy\"])\n",
    "\n",
    "#Here we train our model.\n",
    "classifier.fit(X_train,y_train,epochs = 100,validation_data = (X_test,y_test))\n",
    "#This the inference phase.We try our model on test data.\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "#With this confusion matrix \n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8921e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This the inference phase.We try our model on test data.\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "#With this confusion matrix \n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd93319",
   "metadata": {},
   "source": [
    "Reference published Article : https://doi.org/10.1007/978-981-13-8798-2_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c7624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
